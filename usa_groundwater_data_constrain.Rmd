---
title: "Constraining USGS Well Water Data"
author: "Annette Hilton"
date: "1/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Attach packages

library(tidyverse)
library(here)
library(lubridate)

# Disable scientific notation 

options(scipen=999)

```

```{r}

# Read in the full dataset for all well water measurements for the USA 

entire_usa_gw <- readr::read_tsv("entire_usgs_usa.txt")

```

```{r}
#----------------------------------------------------------------------------------------------------------
# Constrain dataset by dates 
#----------------------------------------------------------------------------------------------------------

# Remove observations with NA values 

entire_usa_gw_dates <- entire_usa_gw %>% 
  filter(!lev_dt == "NA")

# Removed 539759 observations


```

```{r}
#--------------------------------------------------------------------------------------------------------
# Constrain by number of observations (n) 
#--------------------------------------------------------------------------------------------------------

# Constrain by number of observations (n > 1) 

# entire_usa_gw_1 <- entire_usa_gw_dates %>% 
#   group_by(site_no) %>% 
#   filter( n() > 1)
# 
# # Removed 669256 observations 
# 
# # Constrain by number of observations (n > 30) 
# 
entire_usa_gw_30 <- entire_usa_gw_dates %>%
  group_by(site_no) %>%
  filter( n() > 29)
# 
entire_usa_gw_30_count <- entire_usa_gw_30 %>%
  count(site_no, level_year)
# 
# # Constrain by number of observations ( n > 100) 
# 
entire_usa_gw_100 <- entire_usa_gw_dates %>%
  group_by(site_no) %>%
  filter( n() > 99)

entire_usa_gw_100_count <- entire_usa_gw_100 %>%
  count(site_no, level_year)

```



```{r}
#--------------------------------------------------------------------------------------------------------
# Constrain by number of observations (n) and time-scale 
#--------------------------------------------------------------------------------------------------------

# Constrain by number of observations (n > 30) 

entire_usa_gw_30_d <- entire_usa_gw_dates %>% 
  group_by(site_no) %>% 
  filter( n() > 29) %>% 
  filter(level_year > 1940)

entire_usa_gw_30_count_d <- entire_usa_gw_30 %>% 
  count(site_no, level_year)

# Constrain by number of observations ( n > 100) 

entire_usa_gw_100_d <- entire_usa_gw_dates %>% 
  group_by(site_no) %>% 
  filter( n() > 99) 

entire_usa_gw_100_count_d <- entire_usa_gw_100 %>% 
  count(site_no, level_year)
```


**Need to brainstorm a way to sort by decadal observations**

Thoughts on data visualization 

How would I want to visualize this data? 
Changes over time. Individual sites changes over time? Entire USA changes over time with all sites (scatter)? 

### Pull latitude and longitude for ArcGIS purposes 

```{r}
#---------------------------------------------------------------------------------------------------------
# Latitude and Longitude data
#---------------------------------------------------------------------------------------------------------

# Separate latitude and longitude in separate dataframe 
# Remove duplicate site values (only keep each site once)

lat_long_entire_usa_s <- entire_usa_gw %>% 
  select(site_no, dec_lat_va, dec_long_va, state) %>% 
  distinct(site_no, .keep_all = TRUE)

lat_long_entire_usa_s$site_no <- as.numeric(lat_long_entire_usa_s$site_no)


# Write resulting dataframe to tsv 

# write_tsv(lat_long_entire_usa_s, here::here("lat_long_data", "lat_long_entire_usa_s.txt"))

# Same as above but without the state name column 

lat_long_entire_usa <- entire_usa_gw %>% 
  select(site_no, dec_lat_va, dec_long_va) %>% 
  distinct(site_no, .keep_all = TRUE) 

# Write resulting dataframe to tsv 

# write_tsv(lat_long_entire_usa, here::here("lat_long_data", "lat_long_entire_usa.txt"))

```

### Use anti_join() to identify new sites from 2018 to 2019 

```{r}
#---------------------------------------------------------------------------------------------------------
# New sites from 2018 to 2019 
#---------------------------------------------------------------------------------------------------------

# Attempt equivalent of v-lookup in R (anti-join)
# Read in 2018 USGS Data (from Scott Jasechko) 

usgs_2018 <- readr::read_tsv(here::here("lat_long_data", "usgs_2018_data.txt"))

# Use anti-join to find non-matches from the new usgs 2019 data and the usgs 2018 data (new sites) 

# First dataframe: "usgs_2018" 
# Second dataframe: "lat_long_entire_usa_s"

new_usgs_sites <- anti_join(lat_long_entire_usa_s, usgs_2018, by = "site_no")

# Returns dataframe with 640,937 new sites 

# Double check with opposite approach

new_usgs_sites_all_check <- anti_join(entire_usa_gw, usgs_2018, by = "site_no") %>% 
  distinct(site_no, .keep_all = TRUE)

# Write resulting dataframe to tsv file 

write_tsv(new_usgs_sites, here::here("lat_long_data", "new_usgs_sites.txt"))

```

```{r}

# Attempt v-look up but with entire database (not distinct) 

new_usgs_sites_all <- anti_join(entire_usa_gw, usgs_2018, by = "site_no") %>% 
  filter(!lev_dt == "NA")

# Unique (distinct) sites without NA values for dates 

new_usgs_sites_unique <- anti_join(entire_usa_gw, usgs_2018, by = "site_no") %>% 
  filter(!lev_dt == "NA") %>% 
  distinct(site_no, .keep_all = TRUE)

# Attempt to find out new sites date range 

new_usgs_sites_dates <- new_usgs_sites_all %>% 
  group_by(site_no) %>% 
  filter(level_year %in% c(1940:2019))

# Attempt to find out new sites count (per site)

new_usgs_sites_counts <- new_usgs_sites_all %>% 
  group_by(site_no) %>% 
  summarize(
    count = n() 
  )

```

